{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c26809-790e-487d-84a5-dd97f8bdf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62416100-be62-43cc-a72e-186910105d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\" # the compute region for Vertex AI Training and Prediction\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\" # A GCS bucket in the created in the same region.\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7adb0047-c7ea-434a-94f6-f0fbba1903fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968a1ca2-abf1-4f95-ac60-1d467ddeed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.qwiklabs-gcp-04-853e5675f5e8.appspot.com/\n",
      "gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/\n",
      "gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "# List providers, buckets, or objects\n",
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e074f72-5a8a-49d9-b678-435f8fbd2fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3042ff17-9fcc-49a6-9b98-7ead1d402296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'qwiklabs-\n",
      "gcp-04-853e5675f5e8:covertype_dataset' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r2c77a6dee8068435_0000017f87b89e43_1 ... (2s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea005b28-ae73-4b83-baeb-da95033c70e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r45c0e6abeb654c3a_0000017f87b8b120_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# BQ DATASET.TRAIN\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca354708-9f4f-4155-8a19-fa553e76f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/data/training/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "!echo $TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3945bf07-f832-46b0-99ef-4cfdb71b91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r53274c4997de6ad_0000017f87b8c010_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac109f8-ce1d-4f1d-b775-603875afa418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r75893daf2167e15a_0000017f87b8ca53_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# BQ DATASET.VALID\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e724201f-18e3-4830-a10b-f101bb531bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r4165b928cb7dc039_0000017f87b8d8c2_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc7ec16-aeb0-44e6-8429-28f1b8fd4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40009, 13)\n",
      "(9836, 13)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4237e-6a50-4e8a-893d-607aba017771",
   "metadata": {},
   "source": [
    "# 3. Training app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504f8352-0848-404d-bd61-972430340486",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_indexes = slice(0, 10)\n",
    "categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_feature_indexes),\n",
    "        (\"cat\", OneHotEncoder(), categorical_feature_indexes),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", SGDClassifier(loss=\"log\", tol=1e-3)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5117f935-195f-4583-a69e-d8e491c7d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_type_map = {\n",
    "    feature: \"float64\" for feature in df_train.columns[numeric_feature_indexes]\n",
    "}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c42ee93c-f2ad-4f10-870f-d799b85495db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7006913379422529\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(\"Cover_Type\", axis=1)\n",
    "y_train = df_train[\"Cover_Type\"]\n",
    "X_validation = df_validation.drop(\"Cover_Type\", axis=1)\n",
    "y_validation = df_validation[\"Cover_Type\"]\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a5885-7bbd-4140-859e-15833848d19f",
   "metadata": {},
   "source": [
    "# Hyper param app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b1d3a3-f510-4b78-9718-7bfbbecb4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84226503-c644-4e93-8381-d47f334ed7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log',tol=1e-3))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e219876-eb69-4d5e-b17d-9b77884f0219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0cb2d7-2942-42e9-8093-5cdf45617f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4a66c6-2cfa-434f-b1a2-4bddab7b564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1346, in _path_importer_cache\n",
      "KeyError: '/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/site-packages/cryptography/hazmat/bindings'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/../lib/google-cloud-sdk/lib/gcloud.py\", line 132, in <module>\n",
      "    main()\n",
      "  File \"/usr/bin/../lib/google-cloud-sdk/lib/gcloud.py\", line 104, in main\n",
      "    gcloud_main = _import_gcloud_main()\n",
      "  File \"/usr/bin/../lib/google-cloud-sdk/lib/gcloud.py\", line 83, in _import_gcloud_main\n",
      "    import googlecloudsdk.gcloud_main\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/gcloud_main.py\", line 35, in <module>\n",
      "    from googlecloudsdk.calliope import cli\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py\", line 32, in <module>\n",
      "    from googlecloudsdk.calliope import backend\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py\", line 39, in <module>\n",
      "    from googlecloudsdk.calliope import parser_extensions\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/parser_extensions.py\", line 85, in <module>\n",
      "    from googlecloudsdk.core.updater import update_manager\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 41, in <module>\n",
      "    from googlecloudsdk.core.updater import installers\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/core/updater/installers.py\", line 28, in <module>\n",
      "    from googlecloudsdk.core import local_file_adapter\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/core/local_file_adapter.py\", line 25, in <module>\n",
      "    import requests\n",
      "  File \"/usr/bin/../lib/google-cloud-sdk/lib/third_party/requests/__init__.py\", line 95, in <module>\n",
      "    from urllib3.contrib import pyopenssl\n",
      "  File \"/usr/bin/../lib/google-cloud-sdk/lib/third_party/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n",
      "    import OpenSSL.SSL\n",
      "  File \"/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/site-packages/OpenSSL/__init__.py\", line 8, in <module>\n",
      "    from OpenSSL import crypto, SSL\n",
      "  File \"/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/site-packages/OpenSSL/crypto.py\", line 17, in <module>\n",
      "    from OpenSSL._util import (\n",
      "  File \"/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/site-packages/OpenSSL/_util.py\", line 6, in <module>\n",
      "    from cryptography.hazmat.bindings.openssl.binding import Binding\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 914, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1407, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1376, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1348, in _path_importer_cache\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1324, in _path_hooks\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1594, in path_hook_for_FileFinder\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1459, in __init__\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --async --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3401f01-e096-4624-a8c0-038391ff3327",
   "metadata": {},
   "source": [
    "# Tuning job\n",
    "\n",
    "max_iter and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81898ce5-100b-4de7-be95-cd3c9dcbc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"forestcover_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c1a435b-62e8-439f-a74f-e89997e6f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: forestcover_tuning_20220314_091836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [274368283304525824] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 274368283304525824 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: accuracy\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: max_iter\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 10\n",
    "      - 20\n",
    "  - parameterId: alpha\n",
    "    doubleValueSpec:\n",
    "      minValue: 1.0e-4\n",
    "      maxValue: 1.0e-1\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --job_dir=$JOB_DIR\n",
    "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
    "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab970252-ce11-4e7c-bc13-2b0c39132ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4181126e-1480-426b-bc67-b1628e956c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forestcover_tuning_20220314_091836'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c808fd8f-757b-44fd-905d-8a0980422356",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8349/3758979191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_best_trial_from_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOB_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8349/3754359669.py\u001b[0m in \u001b[0;36mretrieve_best_trial_from_job_name\u001b[0;34m(jobname)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_best_trial_from_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8349/3754359669.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(trials)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_measurement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dd117-fba8-47d6-b2fa-7de52a2b62a6",
   "metadata": {},
   "source": [
    "# Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c28fea-b307-4b63-878b-45a58ffeea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = best_trial.parameters[0].value\n",
    "max_iter = best_trial.parameters[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479185b-9a4a-464b-be82-07331b7e130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_dataset_path={TRAINING_FILE_PATH},\\\n",
    "--validation_dataset_path={VALIDATION_FILE_PATH},\\\n",
    "--alpha={alpha},\\\n",
    "--max_iter={max_iter},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ec2a-e746-4434-80b9-98847e1ddda6",
   "metadata": {},
   "source": [
    "# Examine the training output - PKL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80499bf-5453-45ea-bc07-434222b1e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456c141-a323-42ac-b59b-2bdc91ed0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669354a0-e354-4b45-9ce1-6ce6309b83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"forest_cover_classifier_2\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\"\n",
    ")\n",
    "SERVING_MACHINE_TYPE = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f057a-201b-4e46-be78-5b2c2a5d6eec",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d982c-f21d-44e3-80de-1a94bf1f064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=JOB_DIR,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")\n",
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84700cf-4adc-4afe-9d37-93b9a8d28056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "instance = [\n",
    "    2841.0,\n",
    "    45.0,\n",
    "    0.0,\n",
    "    644.0,\n",
    "    282.0,\n",
    "    1376.0,\n",
    "    218.0,\n",
    "    237.0,\n",
    "    156.0,\n",
    "    1003.0,\n",
    "    \"Commanche\",\n",
    "    \"C4758\",\n",
    "]\n",
    "\n",
    "# TODO\n",
    "endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637bec9-f018-4a4c-9768-a6769bfa7a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
