{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80eda7aa-30e9-4c65-97b1-c2502fa6ff78",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "1. Push code [Optional]\n",
    "0. BQ\n",
    "2. Dataset splitting -> Train, Valid -> GCS\n",
    "2. GCS -> Data prep -> GCS\n",
    "2. GCS -> Training starts -> Model upload to bucket\n",
    "3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d7307-5030-45ec-b34c-e50062304be9",
   "metadata": {},
   "source": [
    "# Todos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a093db-ff85-4651-8735-0a0088c2442e",
   "metadata": {},
   "source": [
    "- [ ] CI/CD\n",
    "- [ ] Data Splitting component\n",
    "- [X] Add data preprocessing component\n",
    "- [X] Recommender logic in-place\n",
    "- [ ] Conditional deployment (champion-challenger model)\n",
    "- [ ] Tf data for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543462c-1233-4859-84e8-af906807495d",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d32f9274-a666-4e55-a09e-54d820dc5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "DATASET = \"movielens\"\n",
    "FOLDER = \"match\"\n",
    "PIPELINE_JSON = f\"{FOLDER}/{DATASET}_kfp_pipeline.json\"\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://kfp-{DATASET}-artifact-store-{PROJECT_ID}\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55e92167-38a1-4d33-96d5-81be469d7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_movielens:latest'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_movielens\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef8defd2-8811-423e-9ecb-ad827ddaf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6bc597d8-00ee-42e4-826a-f4e1b0b7c038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATASET=movielens\n",
      "env: DATA_ROOT=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data\n",
      "env: ARTIFACT_STORE=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8\n",
      "env: PIPELINE_ROOT=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/pipeline\n",
      "env: PROJECT_ID=qwiklabs-gcp-04-853e5675f5e8\n",
      "env: REGION=us-central1\n",
      "env: JOB_DIR_ROOT=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/jobs\n",
      "env: TRAINING_FILE_PATH=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/training/dataset.csv\n",
      "env: VALIDATION_FILE_PATH=gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/validation/dataset.csv\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_movielens:latest\n",
      "env: PATH=/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "%env DATASET={DATASET}\n",
    "%env DATA_ROOT={DATA_ROOT}\n",
    "%env ARTIFACT_STORE={ARTIFACT_STORE}\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env JOB_DIR_ROOT={JOB_DIR_ROOT}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "\n",
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0c23110-8361-4acf-9348-7760f3ab61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47785f-06a4-4acd-86de-8eb5a97f138a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa33ca6-174d-43c0-ace2-1d4c641bfc87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1960fc6b-2e28-48d8-a8a7-57762c569a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# DATASET_LOCATION=US\n",
    "# DATASET_ID=covertype_dataset\n",
    "# TABLE_ID=covertype\n",
    "# DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\n",
    "# SCHEMA=Elevation:INTEGER,\\\n",
    "# Aspect:INTEGER,\\\n",
    "# Slope:INTEGER,\\\n",
    "# Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "# Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "# Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "# Hillshade_9am:INTEGER,\\\n",
    "# Hillshade_Noon:INTEGER,\\\n",
    "# Hillshade_3pm:INTEGER,\\\n",
    "# Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "# Wilderness_Area:STRING,\\\n",
    "# Soil_Type:STRING,\\\n",
    "# Cover_Type:INTEGER\n",
    "\n",
    "# bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "# bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "# --source_format=CSV \\\n",
    "# --skip_leading_rows=1 \\\n",
    "# --replace \\\n",
    "# $TABLE_ID \\\n",
    "# $DATA_SOURCE \\\n",
    "# $SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0b28e0-0d46-4dbb-ae93-08e104e9a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8328f4-b6e4-4f4c-9f79-4a1aac9698c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train, Valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4344059-eec3-4719-9092-94f2fad8f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create data prep KFP component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685845e7-4209-4ba2-950f-15003aeb830a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r19b278b7f034f05f_0000017f97ea6b85_1 ... (23s) Current status: DONE   \n",
      "Waiting on bqjob_r4abf94e0b0f8b7c3_0000017f97ead083_1 ... (34s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN DATASET\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table movielens.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `movielens.ratings` AS table \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(table))), 10) IN (1, 2, 3, 4)' \n",
    "\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "movielens.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ab3f83-585a-4924-85e2-67e24f367d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r53f7547101c14ec2_0000017f97eb6143_1 ... (15s) Current status: DONE   \n",
      "Waiting on bqjob_rd4a28b17b21772_0000017f97eba789_1 ... (5s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# CREATE VALID DATASET\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table $DATASET.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `movielens.ratings` AS table \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(table))), 10) IN (8)' \n",
    "\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "$DATASET.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1ddeda-838e-4f77-a26a-f194c5da77d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7996867, 4) Training path: gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/training/dataset.csv\n",
      "\n",
      "(2002053, 4) Validation path: gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/validation/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape, f\"Training path: {TRAINING_FILE_PATH}\\n\")\n",
    "print(df_validation.shape, f\"Validation path: {VALIDATION_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c6439-4bf1-4b0d-913c-9b16fcdd1e16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72150747-f58c-4c3e-86f9-7cd63df1a5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee546793-79a0-4b14-ad2e-7db50ca19826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting match/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/train.py\n",
    "\n",
    "\"\"\"Trainer script.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c666f7b9-038c-4034-9f3f-ea53fc99934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting match/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f07d30-1e02-403e-9b7e-71ce0ee4c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI $FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f3a19-d2fb-42ae-bb8a-207480e47e90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90381e-ca1a-4b50-93f1-27cc4459486e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "df58844b-54b4-4c12-9622-c277dec1ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting match/data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/data.py\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "@component(\n",
    "    # this component builds the recommender model with BigQuery ML\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"tensorflow\", \"tensorflow_datasets\", \"pandas\", \"fsspec\", \"gcsfs\",\"pyarrow\",\"fastparquet\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"data_prep.yaml\"\n",
    ")\n",
    "def data_prep(training_file_path:str,\n",
    "              validation_file_path:str,\n",
    "              training_clean_file_path:str,\n",
    "              validation_clean_file_path:str):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df_train = pd.read_csv(training_file_path)\n",
    "    df_val = pd.read_csv(validation_file_path)\n",
    "\n",
    "    movies_ids = list(set(list(df_train.movieId.unique()) + list(df_val.movieId.unique())))\n",
    "    users_ids = list(set(list(df_train.userId.unique()) + list(df_val.userId.unique())))\n",
    "\n",
    "    dict_movies = dict(zip(movies_ids, range(len(movies_ids)) ))\n",
    "    dict_users = dict(zip(users_ids, range(len(users_ids)) ))\n",
    "\n",
    "    df_train[\"movieId\"] = df_train[\"movieId\"].map(dict_movies)\n",
    "    df_val[\"movieId\"] = df_val[\"movieId\"].map(dict_movies)\n",
    "\n",
    "    df_train[\"userId\"] = df_train[\"userId\"].map(dict_users)\n",
    "    df_val[\"userId\"] = df_val[\"userId\"].map(dict_users)\n",
    "\n",
    "    col = [\"userId\", \"movieId\", \"rating\"]\n",
    "    df_train[col] = df_train[col].astype(np.float32)\n",
    "    df_val[col] = df_val[col].astype(np.float32)\n",
    "\n",
    "    # save to bucket\n",
    "    df_train.to_csv(training_clean_file_path, index=False)\n",
    "    df_val.to_csv(validation_clean_file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85f947-5cbe-437d-b3e0-b0bcc6e8858b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "745dee41-32bb-4dd0-bccc-fd5064a59cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting match/training_lightweight_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/training_lightweight_component.py\n",
    "\n",
    "\"\"\"Lightweight component training function.\"\"\"\n",
    "from kfp.v2.dsl import component\n",
    "import os\n",
    "\n",
    "DATASET = os.environ[\"DATASET\"]\n",
    "\n",
    "@component(\n",
    "    # this component builds the recommender model\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"sklearn\",\"tensorflow\", \"tensorflow_datasets\", \"pandas\", \"fsspec\", \"gcsfs\",\"pyarrow\",\"fastparquet\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_and_fit.yaml\"\n",
    ")\n",
    "def train(training_clean_file_path: str,\n",
    "          validation_clean_file_path: str,\n",
    "          model_dir: str):    \n",
    "    # TODO: Move to train.py script\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    from typing import Dict, Text\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras import optimizers as opt\n",
    "    from tensorflow.keras.layers import Embedding, multiply, concatenate, Flatten, Input, Dense\n",
    "\n",
    "    logging.info(\"Reading train data...\")    \n",
    "    df_train = pd.read_csv(training_clean_file_path)\n",
    "    logging.info(\"Reading valid data...\")\n",
    "    df_val = pd.read_csv(validation_clean_file_path)\n",
    "\n",
    "    num_unique_users=len(set(list(df_train.userId.unique()) + list(df_val.userId.unique())))\n",
    "    num_unique_movies=len(set(list(df_train.movieId.unique()) + list(df_val.movieId.unique())))\n",
    "\n",
    "    users_input = Input(shape=(1,), name=\"users_input\")\n",
    "    users_embedding = Embedding(num_unique_users + 1, 50, name=\"users_embeddings\")(users_input)\n",
    "    users_bias = Embedding(num_unique_users + 1, 1, name=\"users_bias\")(users_input)\n",
    "\n",
    "    movies_input = Input(shape=(1,), name=\"movies_input\")\n",
    "    movies_embedding = Embedding(num_unique_movies + 1, 50, name=\"movies_embedding\")(movies_input)\n",
    "    movies_bias = Embedding(num_unique_movies + 1, 1, name=\"movies_bias\")(movies_input)\n",
    "\n",
    "    dot_product_users_movies = multiply([users_embedding, movies_embedding])\n",
    "    input_terms = dot_product_users_movies + users_bias + movies_bias\n",
    "    input_terms = Flatten(name=\"fl_inputs\")(input_terms)\n",
    "    output = Dense(1, activation=\"relu\", name=\"output\")(input_terms)\n",
    "    model = Model(inputs=[users_input, movies_input], outputs=output)\n",
    "\n",
    "    opt_adam = opt.Adam(lr = 0.005)\n",
    "    model.compile(optimizer=opt_adam, loss= ['mse'], metrics=['mean_absolute_error'])\n",
    "\n",
    "    model.fit(x=[df_train.userId, df_train.movieId], \n",
    "              y=df_train.rating, \n",
    "              batch_size=512, \n",
    "              epochs=1, \n",
    "              verbose=1, \n",
    "              validation_data=([df_val.userId, df_val.movieId], df_val.rating))\n",
    "\n",
    "    # save model\n",
    "    tf.saved_model.save(model, model_dir)\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"joblib\", \"sklearn\", \"google-cloud-bigquery\"],\n",
    "    output_component_file= f\"{DATASET}_kfp_deploy.yaml\",\n",
    ")\n",
    "def deploy(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    serving_container_uri: str,\n",
    "    display_name:str,\n",
    "    artifact_uri:str, \n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    \n",
    "    aiplatform.init(project=project)\n",
    "    \n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name= display_name,\n",
    "        artifact_uri = artifact_uri,\n",
    "        serving_container_image_uri= serving_container_uri\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type=\"n1-standard-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391bbf4-4ac4-4432-8adb-6ca669fde6c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "be9b2964-b594-44f6-a474-922452b4cbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting match/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/pipeline.py\n",
    "\n",
    "\n",
    "import os\n",
    "from kfp import dsl\n",
    "# change the below imports if you change the module name\n",
    "from data import data_prep\n",
    "from training_lightweight_component import train, deploy\n",
    "\n",
    "\n",
    "DATASET = os.getenv(\"DATASET\")\n",
    "DATA_ROOT = os.getenv(\"DATA_ROOT\")\n",
    "ARTIFACT_STORE = os.getenv(\"ARTIFACT_STORE\")\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "TRAINING_CLEAN_FILE_PATH = f\"{DATA_ROOT}/training_clean/dataset.csv\"\n",
    "VALIDATION_CLEAN_FILE_PATH= f\"{DATA_ROOT}/validation_clean/dataset.csv\"\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "MODEL_DIR = f\"{ARTIFACT_STORE}/model\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{DATASET}-kfp-pipeline\",\n",
    "    description=f\"The pipeline training and deploying the {DATASET} model\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    training_container_uri: str = TRAINING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_uri: str = SERVING_CONTAINER_IMAGE_URI,\n",
    "    training_file_path: str = TRAINING_FILE_PATH,\n",
    "    validation_file_path: str = VALIDATION_FILE_PATH,\n",
    "    training_clean_file_path: str = TRAINING_CLEAN_FILE_PATH,\n",
    "    validation_clean_file_path: str = VALIDATION_CLEAN_FILE_PATH,\n",
    "    model_dir: str = MODEL_DIR,\n",
    "    accuracy_deployment_threshold: float = THRESHOLD,\n",
    "    max_trial_count: int = MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count: int = PARALLEL_TRIAL_COUNT,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "):\n",
    "    data_prep_op = data_prep(training_file_path=training_file_path,\n",
    "                             validation_file_path=validation_file_path,\n",
    "                             training_clean_file_path= training_clean_file_path,\n",
    "                             validation_clean_file_path= validation_clean_file_path)\n",
    "    \n",
    "    train_op   = train(training_clean_file_path = training_clean_file_path,\n",
    "                       validation_clean_file_path= validation_clean_file_path,\n",
    "                       model_dir = model_dir)\n",
    "    train_op.after(data_prep_op)\n",
    "    \n",
    "    deploy_op = deploy(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        serving_container_uri=serving_container_uri,\n",
    "        display_name='movie-recommender-keras',\n",
    "        artifact_uri= model_dir\n",
    "    )\n",
    "    deploy_op.after(train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b93aee9-10a5-48b7-ad8d-b668a3c8fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e98757f9-1ca9-41a6-b14d-67c23539b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py {FOLDER}/pipeline.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4c409df9-fe5f-4ed4-bfc6-11e4d64043ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-data-prep\": {\n",
      "        \"executorLabel\": \"exec-data-prep\",\n",
      "        \"inputDefinitions\": {\n",
      "          \"parameters\": {\n",
      "            \"training_clean_file_path\": {\n",
      "              \"type\": \"STRING\"\n",
      "            },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0317 16:23:57.075449896   31284 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb545dc0-6b02-49f3-98b2-3d8c53b0a2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4174eb77-cff3-4af9-82e4-49c8411dae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/movielens-kfp-pipeline-20220317162358?project=1076138843678\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162238 current state:\n",
      "PipelineState.PIPELINE_STATE_CANCELLING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162238\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317162358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=f\"{DATASET}_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "pipeline.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17795a-42e4-4669-bab2-d6dd66939518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
