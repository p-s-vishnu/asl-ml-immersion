{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "091508ca-53ec-4772-a5b6-887eb59a5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from typing import NamedTuple\n",
    "\n",
    "# We'll use this beta library for metadata querying\n",
    "from google.cloud import aiplatform_v1beta1\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4d3e19c-d7ee-4619-90bf-1966d2919b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'qwiklabs-gcp-03-c2cbc3b6d290' # The Project ID\n",
    "BUCKET_NAME = 'qwiklabs-gcp-03-c2cbc3b6d290'\n",
    "\n",
    "%env BUCKET_NAME={BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85f359d1-ca08-4daf-a953-1fec69eb8c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://qwiklabs-gcp-03-c2cbc3b6d290/kfp_tf'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PIPELINE_ROOT = f'gs://{BUCKET_NAME}/kfp_tf' # This is where all pipeline artifacts are sent. You'll need to ensure the bucket is created ahead of time\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "344729c7-07bd-4b8c-b184-6703cc793150",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    # this component builds the recommender model with BigQuery ML\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"tensorflow\", \"tensorflow_datasets\", \"pandas\", \"fsspec\", \"gcsfs\",\"pyarrow\",\"fastparquet\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/data_prep.yaml\"\n",
    ")\n",
    "def data_prep():\n",
    "    import os\n",
    "    import pprint\n",
    "    import tempfile\n",
    "    import pandas as pd\n",
    "\n",
    "    from typing import Dict, Text\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "    # Ratings data.\n",
    "    ratings = pd.read_csv(\"gs://qwiklabs-gcp-03-c2cbc3b6d290-data/ml-25m/ratings.csv\")\n",
    "    # Features of all the available movies.\n",
    "    movies = pd.read_csv(\"gs://qwiklabs-gcp-03-c2cbc3b6d290-data/ml-25m/movies.csv\")\n",
    "    \n",
    "    df = ratings.merge(movies, how=\"left\", on=\"movieId\")\n",
    "    df.to_parquet(\"gs://qwiklabs-gcp-03-c2cbc3b6d290-data/ml-25m/merged/df-rating-movie.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25078bfc-71eb-49fa-910d-a761a937dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    # this component builds the recommender model with BigQuery ML\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"sklearn\",\"tensorflow\", \"tensorflow_datasets\", \"pandas\", \"fsspec\", \"gcsfs\",\"pyarrow\",\"fastparquet\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/train_and_fit.yaml\"\n",
    ")\n",
    "def train_and_fit():    \n",
    "    # TODO: Move to train.py script\n",
    "    import os\n",
    "    import pprint\n",
    "    import tempfile\n",
    "    import pandas as pd\n",
    "\n",
    "    from typing import Dict, Text\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras import optimizers as opt\n",
    "    from tensorflow.keras.layers import Embedding, multiply, concatenate, Flatten, Input, Dense\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "        \n",
    "    df = pd.read_parquet(\"gs://qwiklabs-gcp-03-c2cbc3b6d290-data/ml-25m/merged/df-rating-movie.parquet\")\n",
    "    \n",
    "    df_train, df_val = train_test_split(df, random_state=42, test_size=0.2, stratify=df.rating)\n",
    "    \n",
    "    movies_ids = list(set(list(df_train.movieId.unique()) + list(df_val.movieId.unique())))\n",
    "    users_ids = list(set(list(df_train.userId.unique()) + list(df_val.userId.unique())))\n",
    "\n",
    "    dict_movies = {}\n",
    "    index = 0\n",
    "    for ids in sorted(movies_ids):\n",
    "        dict_movies[ids] = index\n",
    "        index += 1\n",
    "\n",
    "    dict_users = {}\n",
    "    index = 0\n",
    "    for ids in sorted(users_ids):\n",
    "        dict_users[ids] = index\n",
    "        index += 1\n",
    "\n",
    "    df_train[\"movieId\"] = df_train[\"movieId\"].map(dict_movies)\n",
    "    df_val[\"movieId\"] = df_val[\"movieId\"].map(dict_movies)\n",
    "\n",
    "    df_train[\"userId\"] = df_train[\"userId\"].map(dict_users)\n",
    "    df_val[\"userId\"] = df_val[\"userId\"].map(dict_users)\n",
    "    \n",
    "    for col in [\"userId\", \"movieId\", \"rating\"]:\n",
    "        df_train[col] = df_train[col].astype(np.float32)\n",
    "        df_val[col] = df_val[col].astype(np.float32)\n",
    "        \n",
    "    num_unique_users=len(set(list(df_train.userId.unique()) + list(df_val.userId.unique())))\n",
    "    num_unique_movies=len(set(list(df_train.movieId.unique()) + list(df_val.movieId.unique())))\n",
    "    \n",
    "    users_input = Input(shape=(1,), name=\"users_input\")\n",
    "    users_embedding = Embedding(num_unique_users + 1, 50, name=\"users_embeddings\")(users_input)\n",
    "    users_bias = Embedding(num_unique_users + 1, 1, name=\"users_bias\")(users_input)\n",
    "\n",
    "    movies_input = Input(shape=(1,), name=\"movies_input\")\n",
    "    movies_embedding = Embedding(num_unique_movies + 1, 50, name=\"movies_embedding\")(movies_input)\n",
    "    movies_bias = Embedding(num_unique_movies + 1, 1, name=\"movies_bias\")(movies_input)\n",
    "\n",
    "    dot_product_users_movies = multiply([users_embedding, movies_embedding])\n",
    "    input_terms = dot_product_users_movies + users_bias + movies_bias\n",
    "    input_terms = Flatten(name=\"fl_inputs\")(input_terms)\n",
    "    output = Dense(1, activation=\"relu\", name=\"output\")(input_terms)\n",
    "    model = Model(inputs=[users_input, movies_input], outputs=output)\n",
    "    \n",
    "    opt_adam = opt.Adam(lr = 0.005)\n",
    "    model.compile(optimizer=opt_adam, loss= ['mse'], metrics=['mean_absolute_error'])\n",
    "    \n",
    "    model.fit(x=[df_train.userId, df_train.movieId], y=df_train.rating, batch_size=512, epochs=3, verbose=1, validation_data=([df_val.userId, df_val.movieId], df_val.rating))\n",
    "    \n",
    "    OUTPUT_DIR = \"gs://qwiklabs-gcp-03-c2cbc3b6d290/kfp_tf/model\"\n",
    "    #shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    #TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    EXPORT_PATH = os.path.join(OUTPUT_DIR, \"1\")\n",
    "\n",
    "    tf.saved_model.save(model, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5a5b2cb-2b01-47f4-bcf2-d674ec0b8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-03-c2cbc3b6d290\n"
     ]
    }
   ],
   "source": [
    "!(gcloud config get-value project)\n",
    "\n",
    "# 'gs://qwiklabs-gcp-03-c2cbc3b6d290/kfp_tf'\n",
    "# projects/770243501005/locations/us-central1/models/6101273585412734976\n",
    "# h904fd1f45d8737c3p-tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b687368-7651-4540-a540-8dd754d6369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    # Deploys model \n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"joblib\", \"sklearn\", \"xgboost\", \"google-cloud-bigquery\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/deploy_component.yaml\"\n",
    ")\n",
    "def deploy(bucket_name):\n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    \n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name='movie-recommender-keras',\n",
    "        # artifact_uri = f'gs://qwiaklabs-gcp-03-c2cbc3b6d290/kfp_tf/model/5',\n",
    "        artifact_uri = f'gs://{bucket_name}/kfp_tf/model/5',\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afc8c22e-b826-4f03-a091-1af5d886c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88ca41aa-7779-4086-8b1f-8bcf16055436",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/770243501005/locations/us-central1/models/1858319786476306432/operations/8313482802880315392\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/770243501005/locations/us-central1/models/1858319786476306432\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/770243501005/locations/us-central1/models/1858319786476306432')\n"
     ]
    }
   ],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name='movie-recommender-keras',\n",
    "    artifact_uri=f'gs://{BUCKET_NAME}/kfp_tf/model/5/',\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cd870cd-d769-4d29-b95e-f7f632403d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"pipeline-test\",\n",
    "    description='Movie Recommender Keras'\n",
    ")\n",
    "def pipeline(bucket=BUCKET_NAME):\n",
    "    deploy(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "964b5f3b-b5d9-4734-8142-acfd37a08d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "489398ce-71a6-4218-913b-2e1556a2ef83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-test-20220316053126?project=770243501005\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_JSON=\"pipeline.json\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"test-recommender-pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d299aa7-61e5-4038-adb0-2c5ca2c1cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/770243501005/locations/us-central1/pipelineJobs/pipeline-test-20220316053126 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# run = pipeline_jobs.PipelineJob(\n",
    "#     display_name=\"test-pipeine\",\n",
    "#     template_path=\"pipeline.json\",\n",
    "    \n",
    "#     job_id=\"test-{0}\".format(TIMESTAMP),\n",
    "#     enable_caching=False\n",
    "# )\n",
    "\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31077e03-0459-40b2-a7d5-2badfde9d71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
