{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bde52b-5a78-47a3-8ee3-bb9b6cd88d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735e7d53-5102-4ab0-85cd-2371c843d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5393f51-5297-4200-9c34-adc3e157011a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22834c35-e866-480b-97d3-921e58326aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "\n",
    "\n",
    "def train_evaluate(\n",
    "    training_dataset_path, validation_dataset_path, alpha, max_iter\n",
    "):\n",
    "    \"\"\"Trains the Covertype Classifier model.\"\"\"\n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_features = [\n",
    "        \"Elevation\",\n",
    "        \"Aspect\",\n",
    "        \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\",\n",
    "        \"Hillshade_Noon\",\n",
    "        \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\",\n",
    "    ]\n",
    "\n",
    "    categorical_features = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"cat\", OneHotEncoder(), categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", SGDClassifier(loss=\"log\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    num_features_type_map = {feature: \"float64\" for feature in numeric_features}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print(f\"Starting training: alpha={alpha}, max_iter={max_iter}\")\n",
    "    # pylint: disable-next=invalid-name\n",
    "    X_train = df_train.drop(\"Cover_Type\", axis=1)\n",
    "    y_train = df_train[\"Cover_Type\"]\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Save the model\n",
    "    with open(MODEL_FILENAME, \"wb\") as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    subprocess.check_call(\n",
    "        [\"gsutil\", \"cp\", MODEL_FILENAME, AIP_MODEL_DIR], stderr=sys.stdout\n",
    "    )\n",
    "    print(f\"Saved model in: {AIP_MODEL_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8214b248-1619-47ed-84f2-d73afb200c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e25b19-8b69-44e8-b3fd-35d6e25b3f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex:latest'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_covertype_vertex\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d87144-41ca-429e-be56-b1a4313f9571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 92.4 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647340872.262519-8a57a5222203492cb7bd98fb477c71df.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-853e5675f5e8/locations/global/builds/c2776090-1640-4fc4-bddb-cc85c9987260].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c2776090-1640-4fc4-bddb-cc85c9987260?project=1076138843678].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c2776090-1640-4fc4-bddb-cc85c9987260\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647340872.262519-8a57a5222203492cb7bd98fb477c71df.tgz#1647340872639800\n",
      "Copying gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647340872.262519-8a57a5222203492cb7bd98fb477c71df.tgz#1647340872639800...\n",
      "/ [1 files][ 20.2 KiB/ 20.2 KiB]                                                \n",
      "Operation completed over 1 objects/20.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  98.82kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "08c01a0ec47e: Already exists\n",
      "bd48248908bf: Pulling fs layer\n",
      "bff5af70d0ac: Pulling fs layer\n",
      "7a863f90d65e: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "a75f2205d0be: Pulling fs layer\n",
      "8f06f1106c17: Pulling fs layer\n",
      "5a8ebf5e5925: Pulling fs layer\n",
      "4441047ea5de: Pulling fs layer\n",
      "1253e9ba8ab7: Pulling fs layer\n",
      "8b11a2e9d128: Pulling fs layer\n",
      "61bd4f2d2e6f: Pulling fs layer\n",
      "f8d36dcce25d: Pulling fs layer\n",
      "f1b941ed5a20: Pulling fs layer\n",
      "9746114c8daf: Pulling fs layer\n",
      "a8c487052d75: Pulling fs layer\n",
      "030a072533c0: Pulling fs layer\n",
      "59518298f277: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "a75f2205d0be: Waiting\n",
      "8f06f1106c17: Waiting\n",
      "5a8ebf5e5925: Waiting\n",
      "4441047ea5de: Waiting\n",
      "1253e9ba8ab7: Waiting\n",
      "8b11a2e9d128: Waiting\n",
      "61bd4f2d2e6f: Waiting\n",
      "f8d36dcce25d: Waiting\n",
      "f1b941ed5a20: Waiting\n",
      "9746114c8daf: Waiting\n",
      "a8c487052d75: Waiting\n",
      "030a072533c0: Waiting\n",
      "59518298f277: Waiting\n",
      "bd48248908bf: Verifying Checksum\n",
      "bd48248908bf: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "bd48248908bf: Pull complete\n",
      "a75f2205d0be: Verifying Checksum\n",
      "a75f2205d0be: Download complete\n",
      "8f06f1106c17: Verifying Checksum\n",
      "8f06f1106c17: Download complete\n",
      "7a863f90d65e: Verifying Checksum\n",
      "7a863f90d65e: Download complete\n",
      "4441047ea5de: Verifying Checksum\n",
      "4441047ea5de: Download complete\n",
      "5a8ebf5e5925: Verifying Checksum\n",
      "5a8ebf5e5925: Download complete\n",
      "1253e9ba8ab7: Verifying Checksum\n",
      "1253e9ba8ab7: Download complete\n",
      "8b11a2e9d128: Verifying Checksum\n",
      "8b11a2e9d128: Download complete\n",
      "61bd4f2d2e6f: Verifying Checksum\n",
      "61bd4f2d2e6f: Download complete\n",
      "f8d36dcce25d: Verifying Checksum\n",
      "f8d36dcce25d: Download complete\n",
      "9746114c8daf: Verifying Checksum\n",
      "9746114c8daf: Download complete\n",
      "f1b941ed5a20: Verifying Checksum\n",
      "f1b941ed5a20: Download complete\n",
      "a8c487052d75: Verifying Checksum\n",
      "a8c487052d75: Download complete\n",
      "59518298f277: Download complete\n",
      "bff5af70d0ac: Verifying Checksum\n",
      "bff5af70d0ac: Download complete\n",
      "030a072533c0: Verifying Checksum\n",
      "030a072533c0: Download complete\n",
      "bff5af70d0ac: Pull complete\n",
      "7a863f90d65e: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "a75f2205d0be: Pull complete\n",
      "8f06f1106c17: Pull complete\n",
      "5a8ebf5e5925: Pull complete\n",
      "4441047ea5de: Pull complete\n",
      "1253e9ba8ab7: Pull complete\n",
      "8b11a2e9d128: Pull complete\n",
      "61bd4f2d2e6f: Pull complete\n",
      "f8d36dcce25d: Pull complete\n",
      "f1b941ed5a20: Pull complete\n",
      "9746114c8daf: Pull complete\n",
      "a8c487052d75: Pull complete\n",
      "030a072533c0: Pull complete\n",
      "59518298f277: Pull complete\n",
      "Digest: sha256:4dec453a1946d621666e2abf5d1bc46353ec056c9682694a169c2785a57d7bc7\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> bc8479139130\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in aa8e8e64b11a\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 6.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 44.6 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 46.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=49abf68871a9e1d86a1bbecd8d4ea32234a9efec543c495af28c3bb425e55849\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=d3e09d52840703c7f8365f0f2b236ce9efabab498071c25bbbeee62e547f4acf\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=e4a1f43e6ae6da39982d98555c0fb9cf4cf093861a96438c940c442097d1f2d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container aa8e8e64b11a\n",
      " ---> 3d830f2caf56\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 59bdb7e48a6c\n",
      "Removing intermediate container 59bdb7e48a6c\n",
      " ---> cbc45dc86bfa\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 10a7b0207b6c\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 876c69c0dc8d\n",
      "Removing intermediate container 876c69c0dc8d\n",
      " ---> e432705d0543\n",
      "Successfully built e432705d0543\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex]\n",
      "b26cad07c077: Preparing\n",
      "47740f684750: Preparing\n",
      "daa7a99fc028: Preparing\n",
      "b638c3dd1b30: Preparing\n",
      "b107f55e5b0f: Preparing\n",
      "804eb8042115: Preparing\n",
      "6535e7ceaeea: Preparing\n",
      "589d659ee3aa: Preparing\n",
      "5430153ced2f: Preparing\n",
      "8763b90e8bce: Preparing\n",
      "e686d686dc1d: Preparing\n",
      "3b98c56dcfc0: Preparing\n",
      "243bdd09476c: Preparing\n",
      "39e0384be1ed: Preparing\n",
      "ae1f0864cc76: Preparing\n",
      "f1c924e4876e: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9aa8282950fe: Preparing\n",
      "4a1b0b19050d: Preparing\n",
      "c9ffb453bec5: Preparing\n",
      "36ffdceb4c77: Preparing\n",
      "804eb8042115: Waiting\n",
      "6535e7ceaeea: Waiting\n",
      "589d659ee3aa: Waiting\n",
      "5430153ced2f: Waiting\n",
      "39e0384be1ed: Waiting\n",
      "8763b90e8bce: Waiting\n",
      "e686d686dc1d: Waiting\n",
      "ae1f0864cc76: Waiting\n",
      "f1c924e4876e: Waiting\n",
      "3b98c56dcfc0: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "243bdd09476c: Waiting\n",
      "9aa8282950fe: Waiting\n",
      "4a1b0b19050d: Waiting\n",
      "c9ffb453bec5: Waiting\n",
      "36ffdceb4c77: Waiting\n",
      "b638c3dd1b30: Layer already exists\n",
      "b107f55e5b0f: Layer already exists\n",
      "6535e7ceaeea: Layer already exists\n",
      "804eb8042115: Layer already exists\n",
      "589d659ee3aa: Layer already exists\n",
      "5430153ced2f: Layer already exists\n",
      "e686d686dc1d: Layer already exists\n",
      "8763b90e8bce: Layer already exists\n",
      "243bdd09476c: Layer already exists\n",
      "3b98c56dcfc0: Layer already exists\n",
      "39e0384be1ed: Layer already exists\n",
      "ae1f0864cc76: Layer already exists\n",
      "f1c924e4876e: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "9aa8282950fe: Layer already exists\n",
      "4a1b0b19050d: Layer already exists\n",
      "c9ffb453bec5: Layer already exists\n",
      "36ffdceb4c77: Layer already exists\n",
      "b26cad07c077: Pushed\n",
      "47740f684750: Pushed\n",
      "daa7a99fc028: Pushed\n",
      "latest: digest: sha256:d785952a88a43a3274ca0179add5e3c464802327884c2f3707158dc5701a1573 size: 4707\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                        STATUS\n",
      "c2776090-1640-4fc4-bddb-cc85c9987260  2022-03-15T10:41:12+00:00  2M11S     gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647340872.262519-8a57a5222203492cb7bd98fb477c71df.tgz  gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# !gcloud builds submit --async --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI folder\n",
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08dc5f3-26ec-4c71-94b4-d94f784516c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e23c4db-37c0-4f01-97ee-01b3d9047a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8cece1-05d8-4279-9ed8-4c5a541f3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_prebuilt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_prebuilt.py\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.experimental import (\n",
    "    hyperparameter_tuning_job,\n",
    ")\n",
    "from google_cloud_pipeline_components.experimental.custom_job import (\n",
    "    CustomTrainingJobOp,\n",
    ")\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-16\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "# MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "# PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "# THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"covertype\")\n",
    "BASE_OUTPUT_DIR = os.getenv(\"BASE_OUTPUT_DIR\", PIPELINE_ROOT)\n",
    "MODEL_DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"forest-{PIPELINE_NAME}-kfp-pipeline\",\n",
    "    description=\"Kubeflow pipeline that Trains, and deploys on Vertex\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def create_pipeline():\n",
    "\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "                \"args\": [\n",
    "                    f\"--training_dataset_path={TRAINING_FILE_PATH}\",\n",
    "                    f\"--validation_dataset_path={VALIDATION_FILE_PATH}\",\n",
    "                    \"--alpha=1.0e-1\",\n",
    "                    \"--max_iter=1\"\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    training_task = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-training-job\",\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "\n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-model-upload-job\",\n",
    "        artifact_uri=f\"{BASE_OUTPUT_DIR}/model\",\n",
    "        serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    )\n",
    "    model_upload_task.after(training_task)\n",
    "\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-create-endpoint-job\",\n",
    "    )\n",
    "    endpoint_create_task.after(model_upload_task)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8b4a7-8d3c-4a4e-9600-709ee88a87cf",
   "metadata": {},
   "source": [
    "# deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad89cbb0-b357-489c-8571-1c269baee56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/pipeline\n",
      "env: PROJECT_ID=qwiklabs-gcp-04-853e5675f5e8\n",
      "env: REGION=us-central1\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/data/training/dataset.csv\n",
      "env: VALIDATION_FILE_PATH=gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/data/validation/dataset.csv\n",
      "env: BASE_OUTPUT_DIR=gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/models/20220315104429\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BASE_OUTPUT_DIR = f\"{ARTIFACT_STORE}/models/{TIMESTAMP}\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}\n",
    "%env BASE_OUTPUT_DIR={BASE_OUTPUT_DIR}\n",
    "\n",
    "PIPELINE_JSON = \"covertype_kfp_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea27e3c4-5993-4aa4-ae40-b05a6e0b42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-853e5675f5e8-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ce46a8-0d78-4a0a-ae94-07f288945a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipeline_prebuilt.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30929996-704b-4309-b7b8-4b379684cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc60c08c-4530-4360-a438-8ea5e4eda71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/forest-covertype-kfp-pipeline-20220314124519?project=1076138843678\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/1076138843678/locations/us-central1/pipelineJobs/forest-covertype-kfp-pipeline-20220314124519\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"[boilerplate]covertype_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0ef40-fb10-43d6-aa94-985ae5940723",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd161bb-ad9a-4d80-bcec-4dea6f3e5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint = \n",
    "# endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f1a0823-b845-4264-88d3-6bddf2277586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct_value {\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 \"Prediction failed: Exception during sklearn prediction: Specifying the columns using strings is only supported for pandas DataFrames\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"\"Prediction failed: Exception during sklearn prediction: Specifying the columns using strings is only supported for pandas DataFrames\"\n\"\n\tdebug_error_string = \"{\"created\":\"@1647265002.931326419\",\"description\":\"Error received from peer ipv4:142.250.107.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_message\":\"\"Prediction failed: Exception during sklearn prediction: Specifying the columns using strings is only supported for pandas DataFrames\"\\n\",\"grpc_status\":9}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3399/2598984436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mendpoint_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"5729889343875579904\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"us-central1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_3399/2598984436.py\u001b[0m in \u001b[0;36mpredict_custom_trained_model_sample\u001b[0;34m(project, endpoint_id, instances, location, api_endpoint)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     response = client.predict(\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 \"Prediction failed: Exception during sklearn prediction: Specifying the columns using strings is only supported for pandas DataFrames\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Union\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances = instances if type(instances) == list else [instances]\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "    ]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    print(parameters)\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\", dict(prediction))\n",
    "        \n",
    "        \n",
    "        \n",
    "instance = [\n",
    "    2841.0,\n",
    "    45.0,\n",
    "    0.0,\n",
    "    644.0,\n",
    "    282.0,\n",
    "    1376.0,\n",
    "    218.0,\n",
    "    237.0,\n",
    "    156.0,\n",
    "    1003.0,\n",
    "    \"Commanche\",\n",
    "    \"C4758\",\n",
    "]\n",
    "\"\"\"\n",
    "[\n",
    "    { \"Elevation\": \"2841.0\", \"Aspect\":\"45.0\", \"Slope\":\"0.0\", \"Horizontal_Distance_To_Hydrology\":\"644.0\", \"Vertical_Distance_To_Hydrology\":\"282.0\", \"Horizontal_Distance_To_Roadways\":\"1376.0\",\"Hillshade_9am\":\"218.0\", \"Hillshade_Noon\":\"237.0\",  \"Hillshade_3pm\":\"156.0\", \"Horizontal_Distance_To_Fire_Points\":\"1003.0\", \"Wilderness_Area\":\"Commanche\", \"Soil_Type\":\"C4758\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "predict_custom_trained_model_sample(\n",
    "    project=\"qwiklabs-gcp-04-853e5675f5e8\",\n",
    "    endpoint_id=\"5729889343875579904\",\n",
    "    location=\"us-central1\",\n",
    "    instances=[instance]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
