{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d32f9274-a666-4e55-a09e-54d820dc5945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c6439-4bf1-4b0d-913c-9b16fcdd1e16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0c23110-8361-4acf-9348-7760f3ab61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘code/’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee546793-79a0-4b14-ad2e-7db50ca19826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/train.py\n",
    "\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "\n",
    "\n",
    "def train_evaluate(\n",
    "    training_dataset_path, validation_dataset_path, alpha, max_iter, hptune\n",
    "):\n",
    "    \"\"\"Trains the Covertype Classifier model.\"\"\"\n",
    "\n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_features = [\n",
    "        \"Elevation\",\n",
    "        \"Aspect\",\n",
    "        \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\",\n",
    "        \"Hillshade_Noon\",\n",
    "        \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\",\n",
    "    ]\n",
    "\n",
    "    categorical_features = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"cat\", OneHotEncoder(), categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", SGDClassifier(loss=\"log\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    num_features_type_map = {feature: \"float64\" for feature in numeric_features}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print(f\"Starting training: alpha={alpha}, max_iter={max_iter}\")\n",
    "    # pylint: disable-next=invalid-name\n",
    "    X_train = df_train.drop(\"Cover_Type\", axis=1)\n",
    "    y_train = df_train[\"Cover_Type\"]\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        # pylint: disable-next=invalid-name\n",
    "        X_validation = df_validation.drop(\"Cover_Type\", axis=1)\n",
    "        y_validation = df_validation[\"Cover_Type\"]\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print(f\"Model accuracy: {accuracy}\")\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"accuracy\", metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        with open(MODEL_FILENAME, \"wb\") as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        subprocess.check_call(\n",
    "            [\"gsutil\", \"cp\", MODEL_FILENAME, AIP_MODEL_DIR], stderr=sys.stdout\n",
    "        )\n",
    "        print(f\"Saved model in: {AIP_MODEL_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c666f7b9-038c-4034-9f3f-ea53fc99934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55e92167-38a1-4d33-96d5-81be469d7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype:latest'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_covertype\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f07d30-1e02-403e-9b7e-71ce0ee4c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 7.1 KiB before compression.\n",
      "Uploading tarball of [trainer_image_vertex] to [gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647406730.887697-0c28cc59e1a94baa9fc8c93f3e4e8c82.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-853e5675f5e8/locations/global/builds/81560e3c-9a12-4b99-b119-dec78636c00d].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/81560e3c-9a12-4b99-b119-dec78636c00d?project=1076138843678].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"81560e3c-9a12-4b99-b119-dec78636c00d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647406730.887697-0c28cc59e1a94baa9fc8c93f3e4e8c82.tgz#1647406731219997\n",
      "Copying gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647406730.887697-0c28cc59e1a94baa9fc8c93f3e4e8c82.tgz#1647406731219997...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "08c01a0ec47e: Already exists\n",
      "bd48248908bf: Pulling fs layer\n",
      "bff5af70d0ac: Pulling fs layer\n",
      "7a863f90d65e: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "a75f2205d0be: Pulling fs layer\n",
      "8f06f1106c17: Pulling fs layer\n",
      "5a8ebf5e5925: Pulling fs layer\n",
      "4441047ea5de: Pulling fs layer\n",
      "1253e9ba8ab7: Pulling fs layer\n",
      "8b11a2e9d128: Pulling fs layer\n",
      "61bd4f2d2e6f: Pulling fs layer\n",
      "f8d36dcce25d: Pulling fs layer\n",
      "f1b941ed5a20: Pulling fs layer\n",
      "9746114c8daf: Pulling fs layer\n",
      "a8c487052d75: Pulling fs layer\n",
      "030a072533c0: Pulling fs layer\n",
      "59518298f277: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "a75f2205d0be: Waiting\n",
      "8f06f1106c17: Waiting\n",
      "5a8ebf5e5925: Waiting\n",
      "4441047ea5de: Waiting\n",
      "1253e9ba8ab7: Waiting\n",
      "8b11a2e9d128: Waiting\n",
      "61bd4f2d2e6f: Waiting\n",
      "f8d36dcce25d: Waiting\n",
      "f1b941ed5a20: Waiting\n",
      "9746114c8daf: Waiting\n",
      "a8c487052d75: Waiting\n",
      "030a072533c0: Waiting\n",
      "59518298f277: Waiting\n",
      "bd48248908bf: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "bd48248908bf: Pull complete\n",
      "a75f2205d0be: Verifying Checksum\n",
      "a75f2205d0be: Download complete\n",
      "8f06f1106c17: Verifying Checksum\n",
      "8f06f1106c17: Download complete\n",
      "7a863f90d65e: Verifying Checksum\n",
      "7a863f90d65e: Download complete\n",
      "4441047ea5de: Verifying Checksum\n",
      "4441047ea5de: Download complete\n",
      "5a8ebf5e5925: Verifying Checksum\n",
      "5a8ebf5e5925: Download complete\n",
      "1253e9ba8ab7: Verifying Checksum\n",
      "1253e9ba8ab7: Download complete\n",
      "8b11a2e9d128: Verifying Checksum\n",
      "8b11a2e9d128: Download complete\n",
      "f8d36dcce25d: Verifying Checksum\n",
      "f8d36dcce25d: Download complete\n",
      "61bd4f2d2e6f: Verifying Checksum\n",
      "61bd4f2d2e6f: Download complete\n",
      "9746114c8daf: Verifying Checksum\n",
      "9746114c8daf: Download complete\n",
      "f1b941ed5a20: Verifying Checksum\n",
      "f1b941ed5a20: Download complete\n",
      "a8c487052d75: Verifying Checksum\n",
      "a8c487052d75: Download complete\n",
      "59518298f277: Verifying Checksum\n",
      "59518298f277: Download complete\n",
      "bff5af70d0ac: Verifying Checksum\n",
      "bff5af70d0ac: Download complete\n",
      "030a072533c0: Verifying Checksum\n",
      "030a072533c0: Download complete\n",
      "bff5af70d0ac: Pull complete\n",
      "7a863f90d65e: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "a75f2205d0be: Pull complete\n",
      "8f06f1106c17: Pull complete\n",
      "5a8ebf5e5925: Pull complete\n",
      "4441047ea5de: Pull complete\n",
      "1253e9ba8ab7: Pull complete\n",
      "8b11a2e9d128: Pull complete\n",
      "61bd4f2d2e6f: Pull complete\n",
      "f8d36dcce25d: Pull complete\n",
      "f1b941ed5a20: Pull complete\n",
      "9746114c8daf: Pull complete\n",
      "a8c487052d75: Pull complete\n",
      "030a072533c0: Pull complete\n",
      "59518298f277: Pull complete\n",
      "Digest: sha256:4dec453a1946d621666e2abf5d1bc46353ec056c9682694a169c2785a57d7bc7\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> bc8479139130\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 287019f0f6ac\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 2.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 31.3 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 48.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=ddb104a7b1f4690d685c8df5a905fa84a287a968ff5deeb990e6bd61f4f2a0f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=e54a5b39385a5adb3f3c6264fdf695e871c1ab5ffa1a432eb702bf50f09aba3b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=ab575f7491b2668d2802d8ab6b8ae2df361bc11fd84d45a9e9a8b30237f85490\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 287019f0f6ac\n",
      " ---> 3a15e0328638\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in fe12c704daf5\n",
      "Removing intermediate container fe12c704daf5\n",
      " ---> 7cdfdda2024c\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 99c388bf6574\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 844fa14f328d\n",
      "Removing intermediate container 844fa14f328d\n",
      " ---> b428d258e1cb\n",
      "Successfully built b428d258e1cb\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype]\n",
      "347679d5b745: Preparing\n",
      "4fe82c1ed2ef: Preparing\n",
      "f2a497fcc00f: Preparing\n",
      "b638c3dd1b30: Preparing\n",
      "b107f55e5b0f: Preparing\n",
      "804eb8042115: Preparing\n",
      "6535e7ceaeea: Preparing\n",
      "589d659ee3aa: Preparing\n",
      "5430153ced2f: Preparing\n",
      "8763b90e8bce: Preparing\n",
      "e686d686dc1d: Preparing\n",
      "3b98c56dcfc0: Preparing\n",
      "243bdd09476c: Preparing\n",
      "39e0384be1ed: Preparing\n",
      "ae1f0864cc76: Preparing\n",
      "f1c924e4876e: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9aa8282950fe: Preparing\n",
      "4a1b0b19050d: Preparing\n",
      "c9ffb453bec5: Preparing\n",
      "36ffdceb4c77: Preparing\n",
      "804eb8042115: Waiting\n",
      "6535e7ceaeea: Waiting\n",
      "589d659ee3aa: Waiting\n",
      "5430153ced2f: Waiting\n",
      "8763b90e8bce: Waiting\n",
      "e686d686dc1d: Waiting\n",
      "3b98c56dcfc0: Waiting\n",
      "243bdd09476c: Waiting\n",
      "39e0384be1ed: Waiting\n",
      "ae1f0864cc76: Waiting\n",
      "f1c924e4876e: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "9aa8282950fe: Waiting\n",
      "4a1b0b19050d: Waiting\n",
      "c9ffb453bec5: Waiting\n",
      "36ffdceb4c77: Waiting\n",
      "b638c3dd1b30: Layer already exists\n",
      "b107f55e5b0f: Layer already exists\n",
      "804eb8042115: Layer already exists\n",
      "6535e7ceaeea: Layer already exists\n",
      "589d659ee3aa: Layer already exists\n",
      "5430153ced2f: Layer already exists\n",
      "e686d686dc1d: Layer already exists\n",
      "8763b90e8bce: Layer already exists\n",
      "243bdd09476c: Layer already exists\n",
      "3b98c56dcfc0: Layer already exists\n",
      "ae1f0864cc76: Layer already exists\n",
      "39e0384be1ed: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "f1c924e4876e: Layer already exists\n",
      "4a1b0b19050d: Layer already exists\n",
      "9aa8282950fe: Layer already exists\n",
      "36ffdceb4c77: Layer already exists\n",
      "c9ffb453bec5: Layer already exists\n",
      "4fe82c1ed2ef: Pushed\n",
      "347679d5b745: Pushed\n",
      "f2a497fcc00f: Pushed\n",
      "latest: digest: sha256:b77bfec941a126f20cfbf2e51f1fe9f8d94b1acf191efdf724757f1ff09a31ac size: 4707\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                 STATUS\n",
      "81560e3c-9a12-4b99-b119-dec78636c00d  2022-03-16T04:58:51+00:00  2M20S     gs://qwiklabs-gcp-04-853e5675f5e8_cloudbuild/source/1647406730.887697-0c28cc59e1a94baa9fc8c93f3e4e8c82.tgz  gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI trainer_image_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef8defd2-8811-423e-9ecb-ad827ddaf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the serving image\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f3a19-d2fb-42ae-bb8a-207480e47e90",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "745dee41-32bb-4dd0-bccc-fd5064a59cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training_lightweight_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/training_lightweight_component.py\n",
    "\n",
    "\"\"\"Lightweight component training function.\"\"\"\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    output_component_file=\"covertype_kfp_train_and_deploy.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def train_and_deploy(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    container_uri: str,\n",
    "    serving_container_uri: str,\n",
    "    training_file_path: str,\n",
    "    validation_file_path: str,\n",
    "    staging_bucket: str,\n",
    "    alpha: float,\n",
    "    max_iter: int,\n",
    "):\n",
    "\n",
    "    # pylint: disable-next=import-outside-toplevel\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project, location=location, staging_bucket=staging_bucket\n",
    "    )\n",
    "    job = aiplatform.CustomContainerTrainingJob(\n",
    "        display_name=\"model_training\",\n",
    "        container_uri=container_uri,\n",
    "        command=[\n",
    "            \"python\",\n",
    "            \"train.py\",\n",
    "            f\"--training_dataset_path={training_file_path}\",\n",
    "            f\"--validation_dataset_path={validation_file_path}\",\n",
    "            f\"--alpha={alpha}\",\n",
    "            f\"--max_iter={max_iter}\",\n",
    "            \"--nohptune\",\n",
    "        ],\n",
    "        staging_bucket=staging_bucket,\n",
    "        model_serving_container_image_uri=serving_container_uri,\n",
    "    )\n",
    "    model = job.run(replica_count=1, model_display_name=\"covertype_kfp_model\")\n",
    "    endpoint = model.deploy(  # pylint: disable=unused-variable\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type=\"n1-standard-2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "311fe467-3e59-4380-a9df-bc961ed40f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/tuning_lightweight_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/tuning_lightweight_component.py\n",
    "\n",
    "\n",
    "\"\"\"Lightweight component tuning function.\"\"\"\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    output_component_file=\"covertype_kfp_tune_hyperparameters.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def tune_hyperparameters(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    container_uri: str,\n",
    "    training_file_path: str,\n",
    "    validation_file_path: str,\n",
    "    staging_bucket: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [(\"best_accuracy\", float), (\"best_alpha\", float), (\"best_max_iter\", int)],\n",
    "):\n",
    "\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project, location=location, staging_bucket=staging_bucket\n",
    "    )\n",
    "\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_K80\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": container_uri,\n",
    "                \"args\": [\n",
    "                    f\"--training_dataset_path={training_file_path}\",\n",
    "                    f\"--validation_dataset_path={validation_file_path}\",\n",
    "                    \"--hptune\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        display_name=\"covertype_kfp_trial_job\",\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "    )\n",
    "\n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name=\"covertype_kfp_tuning_job\",\n",
    "        custom_job=custom_job,\n",
    "        metric_spec={\n",
    "            \"accuracy\": \"maximize\",\n",
    "        },\n",
    "        parameter_spec={\n",
    "            \"alpha\": hpt.DoubleParameterSpec(\n",
    "                min=1.0e-4, max=1.0e-1, scale=\"linear\"\n",
    "            ),\n",
    "            \"max_iter\": hpt.DiscreteParameterSpec(\n",
    "                values=[1, 2], scale=\"linear\"\n",
    "            ),\n",
    "        },\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "\n",
    "    metrics = [\n",
    "        trial.final_measurement.metrics[0].value for trial in hp_job.trials\n",
    "    ]\n",
    "    best_trial = hp_job.trials[metrics.index(max(metrics))]\n",
    "    best_accuracy = float(best_trial.final_measurement.metrics[0].value)\n",
    "    best_alpha = float(best_trial.parameters[0].value)\n",
    "    best_max_iter = int(best_trial.parameters[1].value)\n",
    "\n",
    "    return best_accuracy, best_alpha, best_max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391bbf4-4ac4-4432-8adb-6ca669fde6c5",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be9b2964-b594-44f6-a474-922452b4cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/pipeline.py\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "# change the below imports if you change the module name\n",
    "from training_lightweight_component import train_and_deploy\n",
    "from tuning_lightweight_component import tune_hyperparameters\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"covertype-kfp-pipeline\",\n",
    "    description=\"The pipeline training and deploying the Covertype classifier\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def covertype_train(\n",
    "    training_container_uri: str = TRAINING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_uri: str = SERVING_CONTAINER_IMAGE_URI,\n",
    "    training_file_path: str = TRAINING_FILE_PATH,\n",
    "    validation_file_path: str = VALIDATION_FILE_PATH,\n",
    "    accuracy_deployment_threshold: float = THRESHOLD,\n",
    "    max_trial_count: int = MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count: int = PARALLEL_TRIAL_COUNT,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "):\n",
    "    staging_bucket = f\"{pipeline_root}/staging\"\n",
    "    # TODO: Remove them later\n",
    "    print(\"training_container_uri:\", training_container_uri)\n",
    "    print(\"serving_container_uri:\", serving_container_uri)\n",
    "    print(\"training_file_path:\", training_file_path)\n",
    "    print(\"validation_file_path:\", validation_file_path)\n",
    "    print(\"accuracy_deployment_threshold:\", accuracy_deployment_threshold)\n",
    "    print(\"pipeline_root:\", pipeline_root)\n",
    "    print(\"staging bucket:\", staging bucket)\n",
    "    \n",
    "    tuning_op = tune_hyperparameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=training_container_uri,\n",
    "        training_file_path=training_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        staging_bucket=staging_bucket,\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    accuracy = tuning_op.outputs[\"best_accuracy\"]\n",
    "\n",
    "    with dsl.Condition(\n",
    "        accuracy >= accuracy_deployment_threshold, name=\"deploy_decision\"\n",
    "    ):\n",
    "        train_and_deploy_op = (  # pylint: disable=unused-variable\n",
    "            train_and_deploy(\n",
    "                project=PROJECT_ID,\n",
    "                location=REGION,\n",
    "                container_uri=training_container_uri,\n",
    "                serving_container_uri=serving_container_uri,\n",
    "                training_file_path=training_file_path,\n",
    "                validation_file_path=validation_file_path,\n",
    "                staging_bucket=staging_bucket,\n",
    "                alpha=tuning_op.outputs[\"best_alpha\"],\n",
    "                max_iter=tuning_op.outputs[\"best_max_iter\"],\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b93aee9-10a5-48b7-ad8d-b668a3c8fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://kfp-artifact-store-proj-qwiklabs-gcp-04-853e5675f5e8/pipeline\n",
      "env: PROJECT_ID=qwiklabs-gcp-04-853e5675f5e8\n",
      "env: REGION=us-central1\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_covertype:latest\n",
      "env: TRAINING_FILE_PATH=gs://kfp-artifact-store-proj-qwiklabs-gcp-04-853e5675f5e8/data/training/dataset.csv\n",
      "env: VALIDATION_FILE_PATH=gs://kfp-artifact-store-proj-qwiklabs-gcp-04-853e5675f5e8/data/validation/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# compile the pipeline\n",
    "ARTIFACT_STORE = f\"gs://kfp-artifact-store-proj-{PROJECT_ID}\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d0b28e0-0d46-4dbb-ae93-08e104e9a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kfp-artifact-store-proj-qwiklabs-gcp-04-853e5675f5e8/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe9ecc5f-e4f3-4bc2-8271-6f1196d6db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"covertype_kfp_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bde8ff66-e34d-43ee-af1a-b6664743b7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipeline_vertex/pipeline.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6f0e80c-5ef9-4bd9-abc9-9114c9e8127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-condition-deploy-decision-1\": {\n",
      "        \"dag\": {\n",
      "          \"tasks\": {\n",
      "            \"train-and-deploy\": {\n",
      "              \"cachingOptions\": {\n",
      "                \"enableCache\": true\n",
      "              },\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb545dc0-6b02-49f3-98b2-3d8c53b0a2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4174eb77-cff3-4af9-82e4-49c8411dae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20220316050253?project=1076138843678\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20220316050253 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"covertype_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968816a-0c21-467b-ada5-6ba817b3a203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
