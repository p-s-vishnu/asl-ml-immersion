{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32f9274-a666-4e55-a09e-54d820dc5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "DATASET = \"movielens\"\n",
    "FOLDER = \"movielens\"\n",
    "PIPELINE_JSON = f\"{FOLDER}/{DATASET}_kfp_pipeline.json\"\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://kfp-{DATASET}-artifact-store-{PROJECT_ID}\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e92167-38a1-4d33-96d5-81be469d7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-853e5675f5e8/trainer_image_movielens:latest'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_movielens\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8defd2-8811-423e-9ecb-ad827ddaf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc597d8-00ee-42e4-826a-f4e1b0b7c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DATASET={DATASET}\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env JOB_DIR_ROOT={JOB_DIR_ROOT}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "\n",
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c23110-8361-4acf-9348-7760f3ab61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47785f-06a4-4acd-86de-8eb5a97f138a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa33ca6-174d-43c0-ace2-1d4c641bfc87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1960fc6b-2e28-48d8-a8a7-57762c569a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# DATASET_LOCATION=US\n",
    "# DATASET_ID=covertype_dataset\n",
    "# TABLE_ID=covertype\n",
    "# DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\n",
    "# SCHEMA=Elevation:INTEGER,\\\n",
    "# Aspect:INTEGER,\\\n",
    "# Slope:INTEGER,\\\n",
    "# Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "# Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "# Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "# Hillshade_9am:INTEGER,\\\n",
    "# Hillshade_Noon:INTEGER,\\\n",
    "# Hillshade_3pm:INTEGER,\\\n",
    "# Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "# Wilderness_Area:STRING,\\\n",
    "# Soil_Type:STRING,\\\n",
    "# Cover_Type:INTEGER\n",
    "\n",
    "# bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "# bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "# --source_format=CSV \\\n",
    "# --skip_leading_rows=1 \\\n",
    "# --replace \\\n",
    "# $TABLE_ID \\\n",
    "# $DATA_SOURCE \\\n",
    "# $SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0b28e0-0d46-4dbb-ae93-08e104e9a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8328f4-b6e4-4f4c-9f79-4a1aac9698c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train, Valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4344059-eec3-4719-9092-94f2fad8f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create data prep KFP component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685845e7-4209-4ba2-950f-15003aeb830a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r31f9b8f23bb02de1_0000017f963a7106_1 ... (23s) Current status: DONE   \n",
      "Waiting on bqjob_r233385397fec6cca_0000017f963ad6cf_1 ... (23s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN DATASET\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table movielens.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `movielens.ratings` AS table \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(table))), 10) IN (1, 2, 3, 4)' \n",
    "\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "movielens.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ab3f83-585a-4924-85e2-67e24f367d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r6a758eb02b9deff9_0000017f963b3bcb_1 ... (15s) Current status: DONE   \n",
      "Waiting on bqjob_r43293434106e0950_0000017f963b813f_1 ... (6s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# CREATE VALID DATASET\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table $DATASET.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `movielens.ratings` AS table \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(table))), 10) IN (8)' \n",
    "\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "$DATASET.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1ddeda-838e-4f77-a26a-f194c5da77d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7996867, 4) Training path: gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/training/dataset.csv\n",
      "\n",
      "(2002053, 4) Validation path: gs://kfp-movielens-artifact-store-qwiklabs-gcp-04-853e5675f5e8/data/validation/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape, f\"Training path: {TRAINING_FILE_PATH}\\n\")\n",
    "print(df_validation.shape, f\"Validation path: {VALIDATION_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c6439-4bf1-4b0d-913c-9b16fcdd1e16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72150747-f58c-4c3e-86f9-7cd63df1a5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee546793-79a0-4b14-ad2e-7db50ca19826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing movielens/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/train.py\n",
    "\n",
    "\"\"\"Trainer script.\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "\n",
    "\n",
    "def train_evaluate(\n",
    "    training_dataset_path, validation_dataset_path, alpha, max_iter, hptune\n",
    "):\n",
    "    \"\"\"Trains the Covertype Classifier model.\"\"\"\n",
    "\n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_features = [\n",
    "        \"Elevation\",\n",
    "        \"Aspect\",\n",
    "        \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\",\n",
    "        \"Hillshade_Noon\",\n",
    "        \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\",\n",
    "    ]\n",
    "\n",
    "    categorical_features = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"cat\", OneHotEncoder(), categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", SGDClassifier(loss=\"log\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    num_features_type_map = {feature: \"float64\" for feature in numeric_features}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print(f\"Starting training: alpha={alpha}, max_iter={max_iter}\")\n",
    "    # pylint: disable-next=invalid-name\n",
    "    X_train = df_train.drop(\"Cover_Type\", axis=1)\n",
    "    y_train = df_train[\"Cover_Type\"]\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        # pylint: disable-next=invalid-name\n",
    "        X_validation = df_validation.drop(\"Cover_Type\", axis=1)\n",
    "        y_validation = df_validation[\"Cover_Type\"]\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print(f\"Model accuracy: {accuracy}\")\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"accuracy\", metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        with open(MODEL_FILENAME, \"wb\") as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        subprocess.check_call(\n",
    "            [\"gsutil\", \"cp\", MODEL_FILENAME, AIP_MODEL_DIR], stderr=sys.stdout\n",
    "        )\n",
    "        print(f\"Saved model in: {AIP_MODEL_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c666f7b9-038c-4034-9f3f-ea53fc99934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movielens/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f07d30-1e02-403e-9b7e-71ce0ee4c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI $FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f3a19-d2fb-42ae-bb8a-207480e47e90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "745dee41-32bb-4dd0-bccc-fd5064a59cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movielens/training_lightweight_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/training_lightweight_component.py\n",
    "\n",
    "\"\"\"Lightweight component training function.\"\"\"\n",
    "from kfp.v2.dsl import component\n",
    "import os\n",
    "\n",
    "DATASET = os.environ[\"DATASET\"]\n",
    "SERVING_CONTAINER_IMAGE_URI = os.environ[\"SERVING_CONTAINER_IMAGE_URI\"]\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    output_component_file= f\"{DATASET}_kfp_train_and_deploy.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def train_and_deploy(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    container_uri: str,\n",
    "    serving_container_uri: str,\n",
    "    training_file_path: str,\n",
    "    validation_file_path: str,\n",
    "    staging_bucket: str,\n",
    "    alpha: float,\n",
    "    max_iter: int,\n",
    "):\n",
    "    # pylint: disable-next=import-outside-toplevel\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project, location=location, staging_bucket=staging_bucket\n",
    "    )\n",
    "    job = aiplatform.CustomContainerTrainingJob(\n",
    "        display_name=\"model_training\",\n",
    "        container_uri=container_uri,\n",
    "        command=[\n",
    "            \"python\",\n",
    "            \"train.py\",\n",
    "            f\"--training_dataset_path={training_file_path}\",\n",
    "            f\"--validation_dataset_path={validation_file_path}\",\n",
    "            f\"--alpha={alpha}\",\n",
    "            f\"--max_iter={max_iter}\",\n",
    "            \"--nohptune\",\n",
    "        ],\n",
    "        staging_bucket=staging_bucket,\n",
    "        model_serving_container_image_uri=serving_container_uri,\n",
    "    )\n",
    "    model = job.run(replica_count=1, model_display_name=f\"{DATASET}_kfp_model\")\n",
    "    endpoint = model.deploy(  # pylint: disable=unused-variable\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type=\"n1-standard-2\",\n",
    "    )\n",
    "    \n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"joblib\", \"sklearn\", \"xgboost\", \"google-cloud-bigquery\"],\n",
    "    output_component_file= f\"{DATASET}_kfp_deploy.yaml\",\n",
    ")\n",
    "def deploy(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    serving_container_uri: str,\n",
    "    display_name:str,\n",
    "    artifact_uri:str, \n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    \n",
    "    aiplatform.init(project=project)\n",
    "    \n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name= display_name,\n",
    "        artifact_uri = artifact_uri,\n",
    "        serving_container_image_uri= serving_container_uri\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type=\"n1-standard-4\"\n",
    "    )\n",
    "    # Save data to the output params\n",
    "    # vertex_endpoint.uri = endpoint.resource_name\n",
    "    # vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311fe467-3e59-4380-a9df-bc961ed40f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movielens/tuning_lightweight_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/tuning_lightweight_component.py\n",
    "\n",
    "\n",
    "\"\"\"Lightweight component tuning function.\"\"\"\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import component\n",
    "import os\n",
    "\n",
    "DATASET = os.environ[\"DATASET\"] \n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    output_component_file=f\"{DATASET}_kfp_tune_hyperparameters.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def tune_hyperparameters(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    container_uri: str,\n",
    "    training_file_path: str,\n",
    "    validation_file_path: str,\n",
    "    staging_bucket: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [(\"best_accuracy\", float), (\"best_alpha\", float), (\"best_max_iter\", int)],\n",
    "):\n",
    "\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project, location=location, staging_bucket=staging_bucket\n",
    "    )\n",
    "\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_K80\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": container_uri,\n",
    "                \"args\": [\n",
    "                    f\"--training_dataset_path={training_file_path}\",\n",
    "                    f\"--validation_dataset_path={validation_file_path}\",\n",
    "                    \"--hptune\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        display_name=f\"{DATASET}_kfp_trial_job\",\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "    )\n",
    "\n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name=f\"{DATASET}_kfp_tuning_job\",\n",
    "        custom_job=custom_job,\n",
    "        metric_spec={\n",
    "            \"accuracy\": \"maximize\",\n",
    "        },\n",
    "        parameter_spec={\n",
    "            \"alpha\": hpt.DoubleParameterSpec(\n",
    "                min=1.0e-4, max=1.0e-1, scale=\"linear\"\n",
    "            ),\n",
    "            \"max_iter\": hpt.DiscreteParameterSpec(\n",
    "                values=[1, 2], scale=\"linear\"\n",
    "            ),\n",
    "        },\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "\n",
    "    metrics = [\n",
    "        trial.final_measurement.metrics[0].value for trial in hp_job.trials\n",
    "    ]\n",
    "    best_trial = hp_job.trials[metrics.index(max(metrics))]\n",
    "    best_accuracy = float(best_trial.final_measurement.metrics[0].value)\n",
    "    best_alpha = float(best_trial.parameters[0].value)\n",
    "    best_max_iter = int(best_trial.parameters[1].value)\n",
    "\n",
    "    return best_accuracy, best_alpha, best_max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391bbf4-4ac4-4432-8adb-6ca669fde6c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be9b2964-b594-44f6-a474-922452b4cbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movielens/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {FOLDER}/pipeline.py\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "# change the below imports if you change the module name\n",
    "from training_lightweight_component import train_and_deploy, deploy\n",
    "from tuning_lightweight_component import tune_hyperparameters\n",
    "\n",
    "\n",
    "DATASET = os.getenv(\"DATASET\")\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{DATASET}-kfp-pipeline\",\n",
    "    description=f\"The pipeline training and deploying the {DATASET} model\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    training_container_uri: str = TRAINING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_uri: str = SERVING_CONTAINER_IMAGE_URI,\n",
    "    training_file_path: str = TRAINING_FILE_PATH,\n",
    "    validation_file_path: str = VALIDATION_FILE_PATH,\n",
    "    accuracy_deployment_threshold: float = THRESHOLD,\n",
    "    max_trial_count: int = MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count: int = PARALLEL_TRIAL_COUNT,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "):\n",
    "#     staging_bucket = f\"{pipeline_root}/staging\"\n",
    "    \n",
    "#     tuning_op = tune_hyperparameters(\n",
    "#         project=PROJECT_ID,\n",
    "#         location=REGION,\n",
    "#         container_uri=training_container_uri,\n",
    "#         training_file_path=training_file_path,\n",
    "#         validation_file_path=validation_file_path,\n",
    "#         staging_bucket=staging_bucket,\n",
    "#         max_trial_count=max_trial_count,\n",
    "#         parallel_trial_count=parallel_trial_count,\n",
    "#     )\n",
    "\n",
    "#     accuracy = tuning_op.outputs[\"best_accuracy\"]\n",
    "\n",
    "#     with dsl.Condition(\n",
    "#         accuracy >= accuracy_deployment_threshold, name=\"deploy_decision\"\n",
    "#     ):\n",
    "#         train_and_deploy_op = (  # pylint: disable=unused-variable\n",
    "#             train_and_deploy(\n",
    "#                 project=PROJECT_ID,\n",
    "#                 location=REGION,\n",
    "#                 container_uri=training_container_uri,\n",
    "#                 serving_container_uri=serving_container_uri,\n",
    "#                 training_file_path=training_file_path,\n",
    "#                 validation_file_path=validation_file_path,\n",
    "#                 staging_bucket=staging_bucket,\n",
    "#                 alpha=tuning_op.outputs[\"best_alpha\"],\n",
    "#                 max_iter=tuning_op.outputs[\"best_max_iter\"],\n",
    "#             )\n",
    "#         )\n",
    "    deploy(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        serving_container_uri=serving_container_uri,\n",
    "        display_name='movie-recommender-keras',\n",
    "        artifact_uri=f'gs://{PROJECT_ID}/kfp_tf/model/5'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b93aee9-10a5-48b7-ad8d-b668a3c8fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98757f9-1ca9-41a6-b14d-67c23539b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py {FOLDER}/pipeline.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c409df9-fe5f-4ed4-bfc6-11e4d64043ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-deploy\": {\n",
      "        \"executorLabel\": \"exec-deploy\",\n",
      "        \"inputDefinitions\": {\n",
      "          \"parameters\": {\n",
      "            \"artifact_uri\": {\n",
      "              \"type\": \"STRING\"\n",
      "            },\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb545dc0-6b02-49f3-98b2-3d8c53b0a2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174eb77-cff3-4af9-82e4-49c8411dae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1076138843678/locations/us-central1/pipelineJobs/movielens-kfp-pipeline-20220317073851 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=f\"{DATASET}_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "pipeline.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17795a-42e4-4669-bab2-d6dd66939518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
